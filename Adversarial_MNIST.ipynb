{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Making the Most of your Colab Subscription",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daiweiworking/BenchmarkDeepLearning/blob/master/Adversarial_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKQ4bH7qMGrA"
      },
      "source": [
        "# Making the Most of your Colab Subscription\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMMqmdiYMkvi"
      },
      "source": [
        "## Faster GPUs\n",
        "\n",
        "With Colab Pro you have priority access to our fastest GPUs. For example, you may get a T4 or P100 GPU at times when most users of standard Colab receive a slower K80 GPU. You can see what GPU you've been assigned at any time by executing the following cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23TOba33L4qf"
      },
      "source": [
        "\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "mnist_train = datasets.MNIST(\"./data\", train=True, download=True, transform=transforms.ToTensor())\n",
        "mnist_test = datasets.MNIST(\"./data\", train=False, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "train_idx = mnist_train.targets <= 1\n",
        "mnist_train.data = mnist_train.data[train_idx]\n",
        "mnist_train.targets = mnist_train.targets[train_idx]\n",
        "\n",
        "test_idx = mnist_test.targets <= 1\n",
        "mnist_test.data = mnist_test.data[test_idx]\n",
        "mnist_test.targets = mnist_test.targets[test_idx]\n",
        "\n",
        "train_loader = DataLoader(mnist_train, batch_size = 100, shuffle=True)\n",
        "test_loader = DataLoader(mnist_test, batch_size = 100, shuffle=False)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFcOrXP1fFK_"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# do a single pass over the data\n",
        "def epoch(loader, model, opt=None):\n",
        "    total_loss, total_err = 0.,0.\n",
        "    for X,y in loader:\n",
        "        yp = model(X.view(X.shape[0], -1))[:,0]\n",
        "        loss = nn.BCEWithLogitsLoss()(yp, y.float())\n",
        "        if opt:\n",
        "            opt.zero_grad()\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "        \n",
        "        total_err += ((yp > 0) * (y==0) + (yp < 0) * (y==1)).sum().item()\n",
        "        total_loss += loss.item() * X.shape[0]\n",
        "    return total_err / len(loader.dataset), total_loss / len(loader.dataset)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFW17YJ-gRDX",
        "outputId": "dbe7df08-7917-4da3-9b17-22ff0d143075"
      },
      "source": [
        "model = nn.Linear(784, 1)\n",
        "opt = optim.SGD(model.parameters(), lr=1.)\n",
        "print(\"Train Err\", \"Train Loss\", \"Test Err\", \"Test Loss\", sep=\"\\t\")\n",
        "for i in range(10):\n",
        "    train_err, train_loss = epoch(train_loader, model, opt)\n",
        "    test_err, test_loss = epoch(test_loader, model)\n",
        "    print(*(\"{:.6f}\".format(i) for i in (train_err, train_loss, test_err, test_loss)), sep=\"\\t\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Err\tTrain Loss\tTest Err\tTest Loss\n",
            "0.005843\t0.015825\t0.000946\t0.003009\n",
            "0.001500\t0.005412\t0.000473\t0.002444\n",
            "0.001184\t0.004382\t0.000946\t0.002326\n",
            "0.001026\t0.003839\t0.000946\t0.002264\n",
            "0.000790\t0.003377\t0.000473\t0.002308\n",
            "0.000790\t0.003255\t0.000946\t0.002033\n",
            "0.000947\t0.003038\t0.000946\t0.001981\n",
            "0.000790\t0.002782\t0.000473\t0.002028\n",
            "0.000711\t0.002624\t0.000946\t0.001893\n",
            "0.000869\t0.002565\t0.000946\t0.001873\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "8u_3b8RagVjY",
        "outputId": "f0dbda0e-5acd-4d29-8fa1-f2a0c4e35137"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "X_test = (test_loader.dataset.data.float()/255).view(len(test_loader.dataset),-1)\n",
        "y_test = test_loader.dataset.targets\n",
        "yp = model(X_test)[:,0]\n",
        "idx = (yp > 0) * (y_test == 0) + (yp < 0) * (y_test == 1)\n",
        "plt.imshow(1-X_test[idx][0].view(28,28).numpy(), cmap=\"gray\")\n",
        "plt.title(\"True Label: {}\".format(y_test[idx][0].item()))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'True Label: 1')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sa-IrJS1aRVJ"
      },
      "source": [
        "In order to use a GPU with your notebook, select the Runtime > Change runtime type menu, and then set the hardware accelerator dropdown to GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65MSuHKqNeBZ"
      },
      "source": [
        "## More memory\n",
        "\n",
        "With Colab Pro you have the option to access high-memory VMs when they are available. To set your notebook preference to use a high-memory runtime, select the Runtime > 'Change runtime type' menu, and then select High-RAM in the Runtime shape dropdown.\n",
        "\n",
        "You can see how much memory you have available at any time by running the following code.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1G82GuO-tez"
      },
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"')\n",
        "  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n",
        "  print('re-execute this cell.')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJW8Qi-pPpep"
      },
      "source": [
        "## Longer runtimes\n",
        "\n",
        "All Colab runtimes are reset after some period of time (which is faster if the runtime isn't executing code). While Colab Pro subscribers still have limits, these will be roughly twice the limits for non-subscribers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLlTRcMM_h0k"
      },
      "source": [
        "## Resource limits in Colab Pro\n",
        "\n",
        "Your resources are not unlimited in Colab Pro. To make the most of Colab Pro, please avoid using resources when you don't need them. For example, only use a GPU or high-RAM runtime when required, and close Colab tabs when finished.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mm8FzEidvPs6"
      },
      "source": [
        "## Send us feedback!\n",
        "\n",
        "If you have any feedback for us, please let us know. The best way to send feedback is by using the Help > 'Send feedback...' menu. If you encounter usage limits in Colab Pro and would be interested in a product with higher usage limits, do let us know.\n",
        "\n",
        "If you encounter errors or other issues with billing (payments) for Colab Pro, please email [colab-billing@google.com](mailto:colab-billing@google.com)."
      ]
    }
  ]
}